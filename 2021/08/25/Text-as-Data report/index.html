<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Reddit is made of threads which contain posts generated by the users. Your aim in this task is to predict the sentiment polarity of each post individually. The exercise dataset contains target column">
<meta property="og:type" content="article">
<meta property="og:title" content="Text-as-Data Coursework Report">
<meta property="og:url" content="https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/index.html">
<meta property="og:site_name" content="YUANCONG&#39;s blog">
<meta property="og:description" content="Reddit is made of threads which contain posts generated by the users. Your aim in this task is to predict the sentiment polarity of each post individually. The exercise dataset contains target column">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-08-25T15:12:50.000Z">
<meta property="article:modified_time" content="2021-08-25T15:13:28.082Z">
<meta property="article:author" content="YUANCONG.L">
<meta property="article:tag" content="English">
<meta property="article:tag" content="Work">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Text-as-Data Coursework Report</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="YUANCONG's blog" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" "Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/projects_url">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post " href="/2021/08/31/Another2021p3/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post " href="/2021/08/24/Another2021p2/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top " href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post " href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&text=Text-as-Data Coursework Report"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&is_video=false&description=Text-as-Data Coursework Report"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Text-as-Data Coursework Report&body=Check out this article: https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&name=Text-as-Data Coursework Report&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&t=Text-as-Data Coursework Report"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Q1"><span class="toc-number">1.</span> <span class="toc-text">Q1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Explore-dataset"><span class="toc-number">1.1.</span> <span class="toc-text">Explore dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Preprocessing"><span class="toc-number">1.2.</span> <span class="toc-text">Preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Discussion-of-classifier-performance"><span class="toc-number">1.3.</span> <span class="toc-text">Discussion of classifier performance</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q2"><span class="toc-number">2.</span> <span class="toc-text">Q2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Attempt-parameter-optimisation"><span class="toc-number">2.1.</span> <span class="toc-text">Attempt parameter optimisation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Explore-the-predictions"><span class="toc-number">2.2.</span> <span class="toc-text">Explore the predictions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Q3"><span class="toc-number"></span> <span class="toc-text">Q3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Propose-features"><span class="toc-number">0.1.</span> <span class="toc-text">Propose features</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-validate-and-test-models"><span class="toc-number">0.2.</span> <span class="toc-text">Train, validate and test models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Performance-analysis"><span class="toc-number">0.3.</span> <span class="toc-text">Performance analysis</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Text-as-Data Coursework Report
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">YUANCONG.L</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2021-08-25T15:12:50.000Z" itemprop="datePublished">2021-08-25</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/English/" rel="tag">English</a>, <a class="tag-link-link" href="/tags/Work/" rel="tag">Work</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>Reddit is made of threads which contain posts generated by the users. Your aim in this task is to predict the sentiment polarity of each post individually. The exercise dataset contains target column called “<code>sentiment.polarity</code>” which can take 5 values: “very negative”, “negative”, “neutral”, “positive” and “very positive” (multi-label classification/prediction).</p>
<p>Read dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_json(<span class="string">&quot;https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_train.json&quot;</span>)</span><br><span class="line">validation_data = pd.read_json(<span class="string">&quot;https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_validation.json&quot;</span>)</span><br><span class="line">test_data = pd.read_json(<span class="string">&quot;https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_test.json&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h2><p>Use the text from the reddit posts (Known as “body”) to train classification models using the Scikit Learn package. The labels to predict are the <code>sentiment.polarity</code> for each post. Conduct experiments using the following combinations of classifier models and feature representations:</p>
<ol>
<li><code>Dummy Classifier</code> with <code>strategy=&quot;most_frequent&quot;</code></li>
<li><code>Dummy Classifier</code> with <code>strategy=&quot;stratified&quot;</code></li>
<li><code>LogisticRegression</code> with <code>One-hot vectorization</code></li>
<li><code>LogisticRegression</code> with <code>TF-IDF vectorization</code> (default settings)</li>
<li><code>SVC Classifier</code> with <code>One-hot vectorization</code> (SVM with RBF kernel, default settings)</li>
<li>An ‘interesting’ classifier model and vectorisation of your choice with appropriate pre-processing</li>
</ol>
<p><strong>Results</strong> - For the above classifiers report the classifier accuracy as well as macro-averaged precision, recall, and F1 (to three decimal places). Show the overall results1 obtained by the classifiers on the training and test sets in one table, and highlight the best performance. For the best performing classifier (by macro F1 in test set) Include a bar chart graph with the F1 score for each class - (sentiment polarity labels on x-axis, F1 score on Y axis).</p>
<h3 id="Explore-dataset"><a href="#Explore-dataset" class="headerlink" title="Explore dataset"></a>Explore dataset</h3><p>There are five types of <code>sentiment.ploraity</code> in the dataset: neutral, positive, negative, very positive and very negative. firstly, counts of the <code>sentiment.poalrity</code> columns of each of the three datasets were conducted and the results are shown in the table below:</p>
<table>
<thead>
<tr>
<th></th>
<th>neutral</th>
<th>positive</th>
<th>negative</th>
<th>very positive</th>
<th>very negative</th>
</tr>
</thead>
<tbody><tr>
<td>training set</td>
<td>7679</td>
<td>3231</td>
<td>878</td>
<td>253</td>
<td>97</td>
</tr>
<tr>
<td>validation set</td>
<td>1961</td>
<td>845</td>
<td>215</td>
<td>73</td>
<td>15</td>
</tr>
<tr>
<td>testing set</td>
<td>2514</td>
<td>1102</td>
<td>282</td>
<td>86</td>
<td>32</td>
</tr>
</tbody></table>
<p>As can be seen from the chart, the distribution of species is uneven, especially the very small proportion of very positive and very negative, which may lead to poor training to result in a high FRP for these two species.</p>
<h3 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">nlp = spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line">nlp.remove_pipe(<span class="string">&#x27;tagger&#x27;</span>)</span><br><span class="line">nlp.remove_pipe(<span class="string">&#x27;parser&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># download a stopword list</span></span><br><span class="line">nltk.download(<span class="string">&#x27;stopwords&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spacy_tokenize</span>(<span class="params">string</span>):</span></span><br><span class="line">  tokens = <span class="built_in">list</span>()</span><br><span class="line">  doc = nlp(string)</span><br><span class="line">  <span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    tokens.append(token)</span><br><span class="line">  <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># normalize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">tokens</span>):</span></span><br><span class="line">  normalized_tokens = <span class="built_in">list</span>()</span><br><span class="line">  <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">    normalized = token.text.lower().strip()</span><br><span class="line">    <span class="keyword">if</span> ((token.is_alpha <span class="keyword">or</span> token.is_digit)):</span><br><span class="line">      normalized_tokens.append(normalized)</span><br><span class="line">  <span class="keyword">return</span> normalized_tokens</span><br><span class="line">  <span class="keyword">return</span> normalized_tokens</span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenize and normalize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_normalize</span>(<span class="params">string</span>):</span></span><br><span class="line">  <span class="keyword">return</span> normalize(spacy_tokenize(string))</span><br><span class="line"></span><br><span class="line"><span class="comment">## Pass in the tokenizer as the tokenizer to the vectorizer.</span></span><br><span class="line"><span class="comment">## Create a one-hot encoding vectorizer.</span></span><br><span class="line">one_hot_vectorizer = CountVectorizer(tokenizer=tokenize_normalize, binary=<span class="literal">True</span>)</span><br><span class="line">train_features = one_hot_vectorizer.fit_transform(train_data[<span class="string">&#x27;body&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## This creates input features for our classification on all subsets of our collection.</span></span><br><span class="line">validation_features = one_hot_vectorizer.transform(validation_data[<span class="string">&#x27;body&#x27;</span>])</span><br><span class="line">test_features = one_hot_vectorizer.transform(test_data[<span class="string">&#x27;body&#x27;</span>])</span><br><span class="line"></span><br><span class="line">train_labels = train_data[<span class="string">&#x27;sentiment.polarity&#x27;</span>]</span><br><span class="line">validation_labels = validation_data[<span class="string">&#x27;sentiment.polarity&#x27;</span>]</span><br><span class="line">test_labels = test_data[<span class="string">&#x27;sentiment.polarity&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h3 id="Discussion-of-classifier-performance"><a href="#Discussion-of-classifier-performance" class="headerlink" title="Discussion of classifier performance"></a>Discussion of classifier performance</h3><p>First we define a evaluation summary function to summarise the scores:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluation_summary</span>(<span class="params">description, predictions, true_labels</span>):</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;Evaluation for: &quot;</span> + description)</span><br><span class="line">  precision = precision_score(predictions, true_labels, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">  recall = recall_score(predictions, true_labels, average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">  accuracy = accuracy_score(predictions, true_labels)</span><br><span class="line">  f1_weighted = fbeta_score(predictions, true_labels, <span class="number">1</span>, average=<span class="string">&#x27;weighted&#x27;</span>) <span class="comment">#1 means f_1 measure</span></span><br><span class="line">  f1_macro = fbeta_score(predictions, true_labels, <span class="number">1</span>, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;Classifier &#x27;%s&#x27; has Acc=%0.3f P=%0.3f R=%0.3f F1_w=%0.3f F1_m=%0.3f&quot;</span> % (description,accuracy,precision,recall,f1_weighted,f1_macro))</span><br><span class="line">  <span class="built_in">print</span>(classification_report(predictions, true_labels, digits=<span class="number">3</span>, zero_division = <span class="number">0</span>))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;\nConfusion matrix:\n&#x27;</span>,confusion_matrix(true_labels, predictions))</span><br></pre></td></tr></table></figure>

<ol>
<li><code>Dummy Classifier</code> with <code>strategy=&quot;most_frequent&quot;</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dummy_mf = DummyClassifier(strategy=<span class="string">&#x27;most_frequent&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.625996015936255</span><br><span class="line">Evaluation for: Dummy Majority</span><br><span class="line">Classifier &#x27;Dummy Majority&#x27; has Acc=0.626 P=1.000 R=0.626 F1_w=0.770 F1_m=0.154</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>Dummy Classifier</code> with <code>strategy=&quot;stratified&quot;</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dummy_prior = DummyClassifier(strategy=<span class="string">&#x27;stratified&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.4676294820717131</span><br><span class="line">Evaluation for: Dummy Prior</span><br><span class="line">Classifier &#x27;Dummy Prior&#x27; has Acc=0.462 P=0.463 R=0.462 F1_w=0.462 F1_m=0.190</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><code>LogisticRegression</code> with <code>One-hot vectorization</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(solver=<span class="string">&#x27;saga&#x27;</span>, max_iter = <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: LR onehot</span><br><span class="line">Classifier &#x27;LR onehot&#x27; has Acc=0.748 P=0.787 R=0.748 F1_w=0.763 F1_m=0.476</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><code>LogisticRegression</code> with <code>TF-IDF vectorization</code> (default settings)</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ngram_vectorizer = TfidfVectorizer(tokenizer=tokenize_normalize)</span><br><span class="line">train_features_idf = ngram_vectorizer.fit_transform(train_data[<span class="string">&#x27;body&#x27;</span>])</span><br><span class="line"></span><br><span class="line">validation_features_idf = ngram_vectorizer.transform(validation_data[<span class="string">&#x27;body&#x27;</span>])</span><br><span class="line">test_features_idf = ngram_vectorizer.transform(test_data[<span class="string">&#x27;body&#x27;</span>])</span><br><span class="line"></span><br><span class="line">lr_idf = LogisticRegression(solver=<span class="string">&#x27;saga&#x27;</span>, max_iter = <span class="number">1000</span>)</span><br><span class="line">lr_idf_model = lr_idf.fit(train_features_idf, train_labels)</span><br><span class="line">evaluation_summary(<span class="string">&quot;LR tf-idf&quot;</span>, lr_idf_model.predict(test_features_idf), test_labels)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: LR tf-idf</span><br><span class="line">Classifier &#x27;LR tf-idf&#x27; has Acc=0.741 P=0.853 R=0.741 F1_w=0.780 F1_m=0.356</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><code>SVC Classifier</code> with <code>One-hot vectorization</code> <strong>(SVM with RBF kernel, default settings)</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svc = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: SVC</span><br><span class="line">Classifier &#x27;SVC&#x27; has Acc=0.730 P=0.875 R=0.730 F1_w=0.782 F1_m=0.287</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>An ‘interesting’ classifier model and vectorisation of your choice with appropriate pre-processing</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: KNN</span><br><span class="line">Classifier &#x27;KNN&#x27; has Acc=0.634 P=0.991 R=0.634 F1_w=0.768 F1_m=0.171</span><br></pre></td></tr></table></figure>

<p>As the results show, the classifier <code>LogisticRegression</code> with <code>One-hot vectorization</code> had the highest accuracy and the highest f1 scores under the weighted average and macro average. The generally low f1 scores may be due to the uneven distribution of the sample, and the unweighted mean may be heavily influenced by the distribution of the sample.<br>With respect to F1 (macro averaged), <code>LogisticRegression</code> with <code>One-hot vectorization</code> scored the highest, and below is the bar chart graph with the F1 score for each class. This result also confirms the above assumption.</p>
<h2 id="Q2"><a href="#Q2" class="headerlink" title="Q2"></a>Q2</h2><p>In this task you will improve the effectiveness of the <code>LogisticRegression</code> with <code>TF-IDF</code> vectorisation from Q1.</p>
<ol>
<li><p><strong>Parameter tuning</strong> - Tune the parameters for both the vectoriser and classifier on the validation set (or using CV-fold validation on the train).<br>Your search does <strong>not</strong> need to be exhaustive. Changing all parameters at once is expensive and slow (a full sweep is exponential in the number of parameters). Consider selecting the best parameters sequentially. The resulting tuned model should improve over the baseline TF-IDF model. Report the results in a table with the accuracy, macro-averaged precision, recall, and F1 on the <strong>test data</strong>. Discuss the parameters and values you tried, what helped and what did not and <em>explain why</em> this may be the case.</p>
<ul>
<li>Classifier - <strong>Regularisation</strong> C value (typical values might be powers of 10 (from $10^{-3}$ to $10^5$)</li>
<li>Vectoriser - Parameters: <code>sublinear_tf</code> and <code>max_features</code> (vocabulary size) (in a range None to 50k)</li>
<li>Select another parameter of your choice from the classifier or vectoriser</li>
</ul>
</li>
<li><p><strong>Error analysis</strong> - Manually examine the predictions of your optimised classifier on the test set. Analyse the results for patterns and trends. Hypothesise why common classification errors are made. Report on your error analysis process and summarise your findings.</p>
</li>
</ol>
<p>We have parameters:</p>
<ol>
<li><code>LogisticRegression</code>: <code>C</code> (typical values are powers of 10 (from $10^{-3}$ to $10^5$);</li>
<li><code>TfidfVectorizer</code>: <code>sublinear_tf</code> (False or True) and <code>max_features</code> (in range None to 50k).</li>
</ol>
<h3 id="Attempt-parameter-optimisation"><a href="#Attempt-parameter-optimisation" class="headerlink" title="Attempt parameter optimisation"></a>Attempt parameter optimisation</h3><p>Before the tuning, we have <strong>f1_macro = 0.353</strong>.<br>There we apply grid search aiming to find the best combination of parameters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">prediction_pipeline = Pipeline([</span><br><span class="line">              (<span class="string">&#x27;selector&#x27;</span>, ItemSelector(key=<span class="string">&#x27;body&#x27;</span>)),</span><br><span class="line">              (<span class="string">&#x27;tf-idf&#x27;</span>, TfidfVectorizer()),</span><br><span class="line">              (<span class="string">&#x27;logreg&#x27;</span>, LogisticRegression(solver=<span class="string">&#x27;saga&#x27;</span>, max_iter = <span class="number">1000</span>))</span><br><span class="line">              ])</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;logreg__C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>, <span class="number">10000</span>, <span class="number">100000</span>],</span><br><span class="line">    <span class="string">&#x27;tf-idf__sublinear_tf&#x27;</span>: [<span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">    <span class="string">&#x27;tf-idf__max_features&#x27;</span>: <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">50000</span>, <span class="number">1000</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(prediction_pipeline, param_grid=params, n_jobs=<span class="number">1</span>, verbose=<span class="number">1</span>, scoring=<span class="string">&#x27;f1_macro&#x27;</span>, cv=<span class="number">2</span>)</span><br><span class="line">grid_search.fit(train_data, train_labels)</span><br><span class="line">best_parameters = grid_search.best_estimator_.get_params()</span><br></pre></td></tr></table></figure>

<p>Besides the parameters above, <code>max_df</code> (of <code>TfidfVectorizer</code> in 2 ranges 0 to 1 and greater than 1) are explored to obtain higher score. A grid search was carried out on max_df using the optimal parameters already obtained above.<br>After the grid search, the best combination of parameters as follows is obtained:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Done 1800 out of 1800 | elapsed: 112.3min finished</span><br><span class="line">logreg__C: 100</span><br><span class="line">tf-idf__max_features: 3000</span><br><span class="line">tf-idf__sublinear_tf: False</span><br></pre></td></tr></table></figure>

<p>After the tuning, we have the <strong>f1_macro</strong> increased to <strong>0.541</strong>.</p>
<h3 id="Explore-the-predictions"><a href="#Explore-the-predictions" class="headerlink" title="Explore the predictions"></a>Explore the predictions</h3><p>For text data where the predicted result differs from the true value, it is printed out as below to analyse the exact cause.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print 20 pieces of mispredicted data</span></span><br><span class="line">predicted = prediction_pipeline_tuned.predict(test_data)</span><br><span class="line"></span><br><span class="line">p = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_labels)):</span><br><span class="line">  <span class="keyword">if</span> p &lt; <span class="number">20</span>:</span><br><span class="line">    <span class="keyword">if</span> test_labels[i] != predicted[i]:</span><br><span class="line">      <span class="built_in">print</span>(test_data[<span class="string">&#x27;body&#x27;</span>][i] + <span class="string">&#x27;\n\nTrue label: &#x27;</span> + test_labels[i] + <span class="string">&#x27; Predicted label: &#x27;</span> + predicted[i] + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">      p += <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>


<ul>
<li>Example 1</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Even better, watch a VOD from [MLG Raleigh](http://tv.majorleaguegaming.com/videos/174-wr4-g2-kiwikaki-vs-nadagast-steppes-of-war-mlg-raleigh-starcraft-2)</span><br><span class="line">The games, the casting, the maps... everything was fucking awful.  Amazing that it was just over one year ago.</span><br><span class="line">True label: neutral Predicted label: positive</span><br></pre></td></tr></table></figure>

<p>Analysis of the statements shows that the sentences contain URL, which may have an impact on the predictions. Also, <em>“Amazing that it was just over one year ago.”</em> uses a positive word like amazing but expresses a negative opinion of the discussion, which could be one of the reasons for the wrong prediction.</p>
<ul>
<li>Example 2</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Your name and your post do not correlate</span><br><span class="line">True label: neutral Predicted label: very positive</span><br></pre></td></tr></table></figure>

<p>It can be seen that the above sentence is a text unrelated to the content of the posting, and the model incorrectly predicts it as very positive. One reason for this may be that the number of samples labelled as <em>very positive</em> is very small, resulting in the model’s poor prediction of a samples labelled as labels which are minority. The other reason might be that the sentence itself does not contain enough information to extract enough features.</p>
<h1 id="Q3"><a href="#Q3" class="headerlink" title="Q3"></a>Q3</h1><p>In this task your goal is to add <strong>two features</strong> to (try to) improve <em>sentiment polarity</em> classification performance obtained in Q2.<br>You must implement and describe two new classifier features and add them to the tuned model from Q2. Examples include adding other properties of the posts (or threads), leveraging embedding-based features, different vectorisation approaches, etc. Train the combined model and report accuracy, macro-averaged precision, recall, and F1 on the test data. Include a well-labeled confusion matrix. Discuss the result in reference to Q2 and what helped (or didn’t) and why you think so.</p>
<h3 id="Propose-features"><a href="#Propose-features" class="headerlink" title="Propose features"></a>Propose features</h3><p>In this dataset, in addition to the “body” which contains the content of the postings, there are other features that may be useful for model training.</p>
<ol>
<li><code>title</code>: The title of the post. This is interrelated with body, so it may help in the training of the model.</li>
<li><code>majority_type</code>: the type of thread. May be useful in the analysis of sentiment.</li>
<li><code>author</code>: the username of the poster. The same sentiment polarity may appear when the<br>same user appears.</li>
<li><code>sentiment.subjectivity</code>: the subjectivity of the posting; its value may reflect the sentiment polarity of the user’s statement; for example, looking at the data, it is clear that a lower value may mean that the user tends to be more neutral.<br>The above features will be attempted.</li>
</ol>
<h3 id="Train-validate-and-test-models"><a href="#Train-validate-and-test-models" class="headerlink" title="Train, validate and test models"></a>Train, validate and test models</h3><p>We start by adding single feature from list above to try them out.</p>
<ol>
<li>Add only <code>title</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add only title</span></span><br><span class="line">prediction_pipeline_union = Pipeline([</span><br><span class="line">        (<span class="string">&#x27;union&#x27;</span>, FeatureUnion(</span><br><span class="line">          transformer_list=[</span><br><span class="line">            (<span class="string">&#x27;title&#x27;</span>, Pipeline([</span><br><span class="line">              (<span class="string">&#x27;selector&#x27;</span>, ItemSelector(key=<span class="string">&#x27;title&#x27;</span>)),</span><br><span class="line">              (<span class="string">&#x27;one-hot&#x27;</span>, TfidfVectorizer(norm=<span class="string">&#x27;l1&#x27;</span>)), </span><br><span class="line">              ])),</span><br><span class="line">            (<span class="string">&#x27;body&#x27;</span>, Pipeline([</span><br><span class="line">              (<span class="string">&#x27;selector&#x27;</span>, ItemSelector(key=<span class="string">&#x27;body&#x27;</span>)), </span><br><span class="line">              (<span class="string">&#x27;one-hot&#x27;</span>, TfidfVectorizer(sublinear_tf=<span class="literal">False</span>, max_features=<span class="number">3000</span>, max_df=<span class="number">1200</span>)), </span><br><span class="line">              ])),</span><br><span class="line">        ])</span><br><span class="line">        )</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: LR tf-idf</span><br><span class="line">Classifier &#x27;LR tf-idf&#x27; has Acc=0.736 P=0.750 R=0.736 F1_w=0.742 F1_m=0.523</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Add only <code>author</code></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: LR tf-idf</span><br><span class="line">Classifier &#x27;LR tf-idf&#x27; has Acc=0.738 P=0.776 R=0.738 F1_w=0.752 F1_m=0.501</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Add only <code>majority_type</code></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: LR tf-idf</span><br><span class="line">Classifier &#x27;LR tf-idf&#x27; has Acc=0.750 P=0.757 R=0.750 F1_w=0.753 F1_m=0.530</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>Add only <code>sentiment.subjectivity</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add only sentiment.subjectivity</span></span><br><span class="line"></span><br><span class="line">numeric_features = [<span class="string">&#x27;sentiment.subjectivity&#x27;</span>]</span><br><span class="line">numeric_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">&#x27;imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;median&#x27;</span>)),</span><br><span class="line">    (<span class="string">&#x27;scaler&#x27;</span>, StandardScaler())])</span><br><span class="line"></span><br><span class="line">text_features = [<span class="string">&#x27;body&#x27;</span>]</span><br><span class="line">text_transformer = TfidfVectorizer(sublinear_tf=<span class="literal">False</span>, max_features=<span class="number">3000</span>, max_df=<span class="number">1200</span>)</span><br><span class="line"></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">&#x27;num&#x27;</span>, numeric_transformer, numeric_features),</span><br><span class="line">        (<span class="string">&#x27;tfidf_1&#x27;</span>, text_transformer, <span class="string">&#x27;body&#x27;</span>),],</span><br><span class="line">                    remainder=<span class="string">&#x27;drop&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## Run evaluation with classifier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateClassifier</span>(<span class="params">classif</span>):</span></span><br><span class="line">  clf = Pipeline(steps=[(<span class="string">&#x27;preprocessor&#x27;</span>, preprocessor),</span><br><span class="line">                        (<span class="string">&#x27;classifier&#x27;</span>, classif)])</span><br><span class="line"></span><br><span class="line">  clf.fit(train_data, train_labels)</span><br><span class="line">  y_pred = clf.predict(test_data)</span><br><span class="line">  <span class="built_in">print</span>(metrics.classification_report(test_labels, y_pred, zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">evaluateClassifier(LogisticRegression(solver=<span class="string">&#x27;saga&#x27;</span>, max_iter = <span class="number">1000</span>, C=<span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: LR tf-idf</span><br><span class="line">Classifier &#x27;LR tf-idf&#x27; has Acc=0.790 P=0.780 R=0.790 F1_w=0.790 F1_m=0.600</span><br></pre></td></tr></table></figure>

<p>We can see that with the addition of a single feature, <code>sentiment.subjectivity</code> has the best effect on the model’s effectiveness, raising the f1 score of the model’s macro average from 0.541 to 0.6.<br>Therefore, if two features are to be added, <code>majority_type</code> and <code>subjectivity</code> are probably the best combination of the features listed in the table above.</p>
<ol start="5">
<li>Add two features</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add sentiment.subjectivity and majority_type</span></span><br><span class="line"></span><br><span class="line">numeric_features = [<span class="string">&#x27;sentiment.subjectivity&#x27;</span>]</span><br><span class="line">numeric_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">&#x27;imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;median&#x27;</span>)),</span><br><span class="line">    (<span class="string">&#x27;scaler&#x27;</span>, StandardScaler())])</span><br><span class="line"></span><br><span class="line">text_features = [<span class="string">&#x27;body&#x27;</span>, <span class="string">&#x27;majority_type&#x27;</span>]</span><br><span class="line">text_transformer = TfidfVectorizer(sublinear_tf=<span class="literal">False</span>, max_features=<span class="number">3000</span>, max_df=<span class="number">1200</span>)</span><br><span class="line"></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">&#x27;num&#x27;</span>, numeric_transformer, numeric_features),</span><br><span class="line">        (<span class="string">&#x27;tfidf_1&#x27;</span>, text_transformer, <span class="string">&#x27;body&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;tfidf_2&#x27;</span>, text_transformer, <span class="string">&#x27;majority_type&#x27;</span>)],</span><br><span class="line">                    remainder=<span class="string">&#x27;drop&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## Run evaluation with classifier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateClassifier</span>(<span class="params">classif</span>):</span></span><br><span class="line">  clf = Pipeline(steps=[(<span class="string">&#x27;preprocessor&#x27;</span>, preprocessor),</span><br><span class="line">                        (<span class="string">&#x27;classifier&#x27;</span>, classif)])</span><br><span class="line"></span><br><span class="line">  clf.fit(train_data, train_labels)</span><br><span class="line">  y_pred = clf.predict(test_data)</span><br><span class="line">  <span class="built_in">print</span>(metrics.classification_report(test_labels, y_pred, zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">evaluateClassifier(LogisticRegression(solver=<span class="string">&#x27;saga&#x27;</span>, max_iter = <span class="number">1000</span>, C=<span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Evaluation for: LR tf-idf</span><br><span class="line">Classifier &#x27;LR tf-idf&#x27; has Acc=0.790 P=0.780 R=0.790 F1_w=0.790 F1_m=0.600</span><br></pre></td></tr></table></figure>

<h3 id="Performance-analysis"><a href="#Performance-analysis" class="headerlink" title="Performance analysis"></a>Performance analysis</h3><p>Comparing the two results shows that the combination of added features has improved all the metrics of the model. Also, there is a slight improvement on the aforementioned poor prediction of a minority labels caused by the uneven sample distribution. Specifically, the proportion improved in scores predicting labels <em>very positive</em> and <em>very negative</em> labels is greater than that in scores of label neutral.</p>
<p>-End-</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/projects_url">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Q1"><span class="toc-number">1.</span> <span class="toc-text">Q1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Explore-dataset"><span class="toc-number">1.1.</span> <span class="toc-text">Explore dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Preprocessing"><span class="toc-number">1.2.</span> <span class="toc-text">Preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Discussion-of-classifier-performance"><span class="toc-number">1.3.</span> <span class="toc-text">Discussion of classifier performance</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q2"><span class="toc-number">2.</span> <span class="toc-text">Q2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Attempt-parameter-optimisation"><span class="toc-number">2.1.</span> <span class="toc-text">Attempt parameter optimisation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Explore-the-predictions"><span class="toc-number">2.2.</span> <span class="toc-text">Explore the predictions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Q3"><span class="toc-number"></span> <span class="toc-text">Q3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Propose-features"><span class="toc-number">0.1.</span> <span class="toc-text">Propose features</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-validate-and-test-models"><span class="toc-number">0.2.</span> <span class="toc-text">Train, validate and test models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Performance-analysis"><span class="toc-number">0.3.</span> <span class="toc-text">Performance analysis</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&text=Text-as-Data Coursework Report"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&is_video=false&description=Text-as-Data Coursework Report"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Text-as-Data Coursework Report&body=Check out this article: https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&title=Text-as-Data Coursework Report"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&name=Text-as-Data Coursework Report&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://blog.yuan-cong.com/2021/08/25/Text-as-Data%20report/&t=Text-as-Data Coursework Report"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2021
    YUANCONG.L
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/projects_url">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


</body>
</html>
